{"cells":[{"cell_type":"markdown","metadata":{"id":"XBKUoskgnYQb"},"source":["\n","\n","<img height=\"80px\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/UPM/Escudo/EscUpm.jpg\" align=\"center\" hspace=\"0px\" vspace=\"0px\" style=\"margin-right: 15px\">\n","\n","#### **Course \"Artificial Neural Networks and Deep Learning\" - Universidad Politécnica de Madrid (UPM)**\n","\n","# **Deep Q-Learning for Lunar lander**\n","\n","Este notebook contiene una implementación del algirtmo Deep Q-learning (DQN) para el problema de la cápsula lunar (ver [OpenAI's LunarLander-v2](https://gym.openai.com/envs/LunarLander-v2/)).\n"]},{"cell_type":"markdown","metadata":{"id":"xO1nCRs4JQOK"},"source":["# Organización del cuaderno\n","\n","El cuaderno se divide en las siguientes secciones:\n","- Sección 0: Una introducción acerca del entorno y del problema a resolver.\n","- Sección 1: Contiene todas las funciones necesarias para que se puede ejecutar un modelo en el entorno \"Lunar Lander\". En ella se definen las clases *DQN* y *Memory*, además de diversas funciones para que se pueda mostrar un vídeo de la ejecución.\n","- Sección 2: Contiene las funciones para entrenar un modelo en el entorno \"Lunar Lander\" y guardarlo junto con los vídeos de las ejecuciones.\n","- Sección 3: Permite importar un modelo ya entrenado y ejecutarlo en el entorno \"Lunar Lander\". Como resultado mostrará el score obtenido y la cantidad de transiciones que hay almacenadas en la memoria y un vídeo de la ejecución."]},{"cell_type":"markdown","metadata":{"id":"_cnUTMnx5H2h"},"source":["# 0. Introducción"]},{"cell_type":"markdown","metadata":{"id":"-dPqVQSRFHXH"},"source":["## 0.1. Cómo utilizar el cuaderno\n","\n","En primer lugar se debe ejecutar la sección 1. Dentro de está se encuentra el apartado 1.1, que se encarga de instalar todas las dependencias necesarias, por lo que probablemente sea necesario reiniciar el cuaderno después de la primera ejecución.\n","\n","La ejecución de la sección 2 permite entrenar un nuevo agente. \n","\n","Si se desea importar y ejectuar un modelo ya existente, debe ejecutarse la sección 3."]},{"cell_type":"markdown","metadata":{"id":"TlOVXqNoNKyh"},"source":["## 0.2. Entorno Lunar lander"]},{"cell_type":"markdown","metadata":{"id":"C3tNcL8wnkWd"},"source":["El \"Lunar lander simulator\" de [Open Ai Gym](https://gym.openai.com/envs/LunarLander-v2/) tiene un entorno con las siguientes características:\n","\n","**Código de lunar_lander.py:**\n","https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py#L258\n","\n","\n","**State vector** (observaciones):\n","- state[0]: Coordenada en el eje X\n","- state[1]: Coordenada en el eje Y\n","- state[2]: Velocidad horizontal\n","- state[3]: Velocidad vertical\n","- state[4]: Ángulo\n","- state[5]: Velocidad angular\n","- state[6]: Contacto de la pierna izquierda\n","- state[7]: Contacto de la pierna derecha\n","\n","**Actions**:\n","- 0 No hacer nada\n","- 1 Encender el motor principal (empujar hacia arriba)\n","- 2 Encender el motor izquierdo motor izquierdo (empujar a la derecha)\n","- 3 Encender el motor derecho (empujar a la izquierda)\n","\n","**Información adicional**:\n","- **Inicio**: El vehículo parte de la parte superior de la pantalla (con velocidad inicial aleatoria) y la plataforma de aterrizaje está siempre en las coordenadas (0,0).\n","- **Recompensa**: \n","\n","    -**Según el estado actual**\n","      shaping = (\n","                -100 * sqrt(x^2 + y^2)       # Distancia al centro, (0,0)\n","                -100 * sqrt(Vy^2 + Vx^2)     # Cuánto se aleja de tener velocidad nula\n","                -100 * abs(ángulo)           # Penalizando cualquier ángulo de la cápsula que no sea el neutro\n","                +10 * contacto pierna izquierda\n","                +10 * contacto pierna derecha\n","                )\n","        \n","        # Esta resta se hace para no acumular las recompensas y que en cada\n","        # iteración tengamos únicamente el de la propia operación.\n","        reward = shaping - prev_shaping\n","    -**Según el combustible usado**\n","      reward -= m_power * 0.3    # Main engine\n","      reward -= s_power * 0.03   # Side engine\n","\n","    -**Según el estado final de la nave**\n","      if game_over o se sale del mapa (creo):\n","        reward = -100\n","      if aterriza\n","        reward = +100"]},{"cell_type":"markdown","metadata":{"id":"AgA79jbcNRng"},"source":["##0.3. Modificaciones al modelo"]},{"cell_type":"markdown","metadata":{"id":"6UgOQUG6NVb6"},"source":["### 1. Relativas a la estructura de DQN\n","\n","  En este apartado se van a explicar los cambios realizados en la clase DQN.\n","\n","####  1.1 Target Network\n","\n","  El uso de una target network en DQN proporciona estabilidad en el entrenamiento. Esta red tiene la misma estructura que la utilizada para el aprendizaje, con la diferencia de que los pesos de esta no se actualizan en cada paso, si no que se realiza cada k número de iteraciones. \n","\n","  El objetivo de esta práctica es utilizar valores objetivo más estables, es decir, que no varíen tanto debido al aprendizaje de la propia red, que los actualiza en cada iteración.\n","\n","####  1.2 Discretización de los estados\n","\n","  Esta modificación es relativa al entorno que estamos tratando y a la manera en que el agente intereactúa con él. La información que recibe el agente del entorno son valores reales relativos a la posición, velocidad e inclinación, y estos números tiene una precisión con bastantes decimales. El problema surge cuando se generan una gran cantidad de diferentes estados, lo que aumenta en gran medida la dificultad para el aprendizaje de la red.\n","\n","Una solución a este problema pasa por discretizar esos valores y acotarlos en un determinado rango. Nosotros hemos decidido discretizar estos números en intervalos de 0.5. \n","\n","Por otro lado, hemos acotado el rango en el que están definidas estas mediciones del entorno. Por ejemplo, en el caso de la posición, se ha determinado una cierta región fuera de la cual ya no es relevante si la cápsula se encuentra más lejos o más cerca, devolviendo un valor fijo y consiguiendo así que el agente únicamente esté manejando un estado para cualquier posición en ese rango, teniendo que procesar menos información.\n","\n","A continuación se incluye el rango posible originalmente para cada una de las variables de un estado y el rango en el que se han acotado. \n","\n","    - Posición x:\n","      \n","      Inicial = [-1, 1]     \n","      Acotado = [-0.5, 0.5]\n","    \n","    - Posición y:\n","    \n","      Inicial = [-2, 10]  \n","      Acotado = [5, -1]\n","    \n","    - Velocidad x:\n","      \n","      Inicial = [-4, 4]  \n","      Acotado = [-1, 1]\n","\n","    - Velocidad y:\n","\n","      Inicial = [-4, 4]  \n","      Acotado = [-1, 1]\n","    \n","    - Ángulo:\n","\n","      Inicial = [-3, 3]  \n","      Acotado = [-1, 1]\n","\n","    - Velocidad angular:\n","\n","      Inicial = [-3, 3]  \n","      Acotado = [-1, 1]\n","\n","\n","### 2. Relativas a los hiperparámetros:\n","- **Batch**: Es el número de ejemplos que procesará el modelo antes de actualizar los pesos de la red neuronal. Se han probado configuraciones de 256, 128, 64 y 32, obteniéndose los mejores resultados para un tamaño de batch de 64. Por temas de estructura de computadores es recomendable emplear tamaños de batch de potencias de dos y por eso no se han probado valores fuera de esta restricción.\n","- **Exploration Decay**: Los modelos de aprendizaje por refuerzo se basan en un balance entre exploración, es decir, la selección una acción de forma aleatoria para conocer mejor el entorno, y explotación, que consiste en seleccionar una acción que trate de maximizar la recompensa. Se han probado configuraciones comprendidas entre el rango 0.99 y 1, e incluso se probó a planificar por episodio,reduciendo a la mitad la tasa de exploración cada 50 época comenzando por 0.5. Sin embargo, los mejores resultados se han conseguido con un Exploration Decay de 0.9998.\n","- **Gamma**: Es el factor de descuento. Cuantifica la importancia que damos a las recompensas futuras. Gamma varía de 0 a 1. Si Gamma está más cerca de cero, el agente tenderá a considerar sólo las recompensas inmediatas. Si Gamma está más cerca de uno, el agente considerará las recompensas futuras con mayor peso, dispuesto a retrasar la recompensa. En nuestro caso, dado que el objetivo final es aterrizar la nave de forma adecuada, hemos decidido mantenerlo en 0.99, siendo las recompensas intermedias menos relevantes.\n","- **Neurons**: Es el número de nodos por capa que tendrá nuestra red o modelo. La capa de entrada viene determinada por 8 nodos, cada uno de ellos representa una variable perteneciente al estado en el que se encuentra la nave a aterrizar. Por su parte, la capa de salida corresponde a cada una de las 4 acciones que pueden ejecutarse en el entorno, por lo que serán 4 neuronas. En cuanto a las capas intermedias, se han probado multitud de configuraciones como : [256, 128, 64], [128, 128], [128, 64, 32], [64, 32], sin embargo la que mejores resultados ha logrado ha sido [128, 64, 32].\n","- **Update_w**: Establece cada cuantos pasos se van a copiar los pesos del modelo que esta aprendiendo sobre el modelo target. Nuevamente se han probado múltiples configuraciones, alcanzándose los mejores resultados con un valor de 100. \n","- **Learning rate (lr)**: Determina en qué grado varían los pesos de las neuronas durante la fase de retropropagación del gradiente. En general hemos observado que un lr relativamente pequeños se adaptan mejor a este problema, tras numerosas pruebas, se ha determinado como bueno para el lr el intervalo comprendido entre 0.001 y 0.003.\n","- **Intermediate activations**: Determina la función de activación utilizada en las capas intermedias de la red. Se ha comprobado que ReLu ha logrado los mejores resultados.\n","- **Out activation**: Determina la función de activación utilizada en la capa de salida de la red. Han sido probadas las funciones Softmax y Linear, siendo la segunda la que ha logrado mejores resultados."]},{"cell_type":"markdown","metadata":{"id":"aLSzCES5NG80"},"source":["## 0.4. Paths\n"]},{"cell_type":"markdown","metadata":{"id":"m-FsjbqK-4Jv"},"source":["Rutas al directorio en drive donde se está ejecutando el cuaderno y nombre de este. Cambiar la variable *path* para que coincida con su ruta y la variable *dir* con el nombre del directio donde está el cuaderno y el agente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6ogXfGKHKUF"},"outputs":[],"source":["path = '/content/drive/MyDrive/Practica-2/Modelos/'\n","\n","dir = \"Entrega\"\n","res_path = path + dir + \"/\""]},{"cell_type":"markdown","metadata":{"id":"zNDrVZNUFMvV"},"source":["# 1. Funciones generales para el funcionamiento del modelo en el entorno \"LunarLander\"\n","\n","En este apartado se define todo lo necesario para que se puede ejecutar un modelo en el entorno \"LunarLander\". Es necesario ejecutar esta sección tanto si se quiere entrenar un modelo (sección 2) o si se quiere importar uno ya entrenado (sección 3)."]},{"cell_type":"markdown","metadata":{"id":"E96PZqWElzG-"},"source":["## 1.1. Instalación de dependencias y librerias para poder mostrar el video y que el entorno funcione adecuadamente\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Baz_NIOI8jDJ","outputId":"baea7aa0-5274-466b-f57f-bd5f0d5198ff","executionInfo":{"status":"ok","timestamp":1642371068053,"user_tz":-60,"elapsed":25070,"user":{"displayName":"Carlos Golvin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJZsuCiO1EHaYSgOJpZy8C1ZoD20MAJv32VJx5hA=s64","userId":"00576658845201820777"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Waiting for headers] [Co\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r                                                                    \rIgn:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","freeglut3-dev is already the newest version (2.8.1-3).\n","ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n","0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n","Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.5)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n","Requirement already satisfied: imageio==2.4.0 in /usr/local/lib/python3.7/dist-packages (2.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (1.21.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (7.1.2)\n","Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (2.2)\n","Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n"]}],"source":["!sudo apt-get update\n","!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!apt-get update > /dev/null 2>&1\n","\n","!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","!pip install 'imageio==2.4.0'\n","!pip install pyvirtualdisplay\n","!pip install numpy --upgrade"]},{"cell_type":"markdown","metadata":{"id":"QsqqfRzKnaYd"},"source":["## 1.2. Importación de librerías y montado del Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAC6RHeIl6L_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642371084442,"user_tz":-60,"elapsed":16398,"user":{"displayName":"Carlos Golvin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJZsuCiO1EHaYSgOJpZy8C1ZoD20MAJv32VJx5hA=s64","userId":"00576658845201820777"}},"outputId":"996b224c-9933-42dc-dbe0-66b410c7a900"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Sun Jan 16 22:11:24 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["import gym\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","import time\n","import random\n","import os\n","\n","\n","import pandas as pd\n","from tensorflow.keras import initializers\n","from keras.utils.vis_utils import plot_model\n","from keras.layers.core import Activation\n","from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n","import pickle\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Imports para la generación de vídeos\n","from gym.wrappers import Monitor\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from pyvirtualdisplay import Display\n","from IPython import display as ipythondisplay\n","import imageio\n","import IPython\n","import PIL.Image\n","import pyvirtualdisplay\n","\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"xFQAPTogOM6w"},"source":["## 1.3. Funciones para mostrar las simulaciones de cada episodio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKB_3Xru2sTD"},"outputs":[],"source":["display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n","def show_video(video):\n","  #mp4list = glob.glob('video/*.mp4')\n","  if len(video) > 0:\n","    video = io.open(video, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","\n","def mostrar_videos():\n","  videos = []\n","  lista_videos = os.listdir(res_path_vid)\n","  for video in lista_videos:\n","    if video.endswith('.mp4'):\n","      videos.append(res_path_vid + video)\n","  for video in range(len(videos)):\n","    print(\"Episodio: \" + str(video + 1))\n","    show_video(videos[video])\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"]},{"cell_type":"markdown","metadata":{"id":"9Cux_BkCMaJ6"},"source":["## 1.4. Arquitectura del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArgtsHCLpTUY"},"outputs":[],"source":["def construir_modelo(inputs, outputs):\n","  model = keras.Sequential(name=dir)\n","  model.add(keras.layers.InputLayer(input_shape=(inputs,), batch_size=None))\n","\n","  for neurons in NEURONS_LAYER:\n","    model.add(keras.layers.Dense(neurons, kernel_initializer=keras.initializers.he_normal, use_bias=True))\n","    model.add(Activation(ACT_INTS))\n","\n","  model.add(keras.layers.Dense(number_of_actions, activation=ACT_OUT))\n","  model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n","  \n","  return model"]},{"cell_type":"markdown","metadata":{"id":"rLv-CeVtnfPC"},"source":["## 1.5. Clase Memoria\n","\n","Memoria para guardar las transiciones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Vg3wMTDl_yK"},"outputs":[],"source":["class ReplayMemory:\n","\n","    def __init__(self, number_of_observations):\n","        # Create replay memory\n","        self.states = np.zeros((MEMORY_SIZE, number_of_observations))\n","        self.states_next = np.zeros((MEMORY_SIZE, number_of_observations))\n","        self.actions = np.zeros(MEMORY_SIZE, dtype=np.int32)\n","        self.rewards = np.zeros(MEMORY_SIZE)\n","        self.terminal_states = np.zeros(MEMORY_SIZE, dtype=bool)\n","        self.current_size=0\n","\n","    def store_transition(self, state, action, reward, state_next, terminal_state):\n","        # Store a transition (s,a,r,s') in the replay memory\n","        i = self.current_size\n","        self.states[i] = state\n","        self.states_next[i] = state_next\n","        self.actions[i] = action\n","        self.rewards[i] = reward\n","        self.terminal_states[i] = terminal_state\n","        self.current_size = i + 1\n","\n","    def sample_memory(self, batch_size):\n","        # Generate a sample of transitions from the replay memory\n","        batch = np.random.choice(self.current_size, batch_size)\n","        states = self.states[batch]\n","        states_next = self.states_next[batch]\n","        rewards = self.rewards[batch]\n","        actions = self.actions[batch]   \n","        terminal_states = self.terminal_states[batch]  \n","        return states, actions, rewards, states_next, terminal_states"]},{"cell_type":"markdown","metadata":{"id":"eZGWyMtinhUC"},"source":["## 1.6. Clase DQN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pADOV6EmBf2"},"outputs":[],"source":["class DQN:\n","\n","    def __init__(self, number_of_observations, number_of_actions):\n","        # Initialize variables and create neural model\n","        self.exploration_rate = EXPLORATION_MAX\n","        self.number_of_actions = number_of_actions\n","        self.number_of_observations = number_of_observations\n","        self.scores = []\n","        self.memory = ReplayMemory(number_of_observations)\n","        self.model = construir_modelo(number_of_observations, number_of_actions)\n","        self.model_target = construir_modelo(number_of_observations, number_of_actions)\n","        self.update_target_weights()\n","\n","    def remember(self, state, action, reward, next_state, terminal_state):\n","        # Store a tuple (s, a, r, s') for experience replay\n","        state = np.reshape(state, [1, self.number_of_observations])\n","        next_state = np.reshape(next_state, [1, self.number_of_observations])\n","        self.memory.store_transition(state, action, reward, next_state, terminal_state)\n","\n","    def select(self, state):\n","        # Generate an action for a given state using epsilon-greedy policy\n","        if np.random.rand() < self.exploration_rate: #Aqui simplemente ve si hace una accion aleatoria o no, como exploration_max es 1, es determinista, siempre va a hacer la predicha por el modelo\n","            return random.randrange(self.number_of_actions)\n","        else:\n","            state = np.reshape(state, [1, self.number_of_observations])\n","            q_values = self.model.predict(state)\n","            return np.argmax(q_values[0])\n","\n","    def learn(self):\n","        # Learn the value Q using a sample of examples from the replay memory\n","        if self.memory.current_size < BATCH_SIZE: return\n","\n","        states, actions, rewards, next_states, terminal_states = self.memory.sample_memory(BATCH_SIZE)\n","\n","        q_targets = self.model.predict(states)\n","        q_next_states = self.model_target.predict(next_states)\n","\n","        for i in range(BATCH_SIZE):\n","             if (terminal_states[i]):\n","                  q_targets[i][actions[i]] = rewards[i]\n","             else:\n","                  q_targets[i][actions[i]] = rewards[i] + GAMMA * np.max(q_next_states[i])    \n","\n","        self.model.train_on_batch(states, q_targets)\n","\n","        # Decrease exploration rate\n","        self.exploration_rate *= EXPLORATION_DECAY\n","        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n","\n","    def update_target_weights(self):\n","        self.model_target.set_weights(self.model.get_weights())\n","\n","    def add_score(self, score):\n","       # Add the obtained score in a list to be presented later\n","        self.scores.append(score)\n","\n","    def display_scores_graphically(self):\n","        # Display the obtained scores graphically\n","        plt.plot(self.scores)\n","        plt.xlabel(\"Episode\")\n","        plt.ylabel(\"Score\")\n","\n","    def get_resultados(self):\n","        return self.scores\n","      \n","    def get_modelo(self):\n","        return self.model"]},{"cell_type":"markdown","metadata":{"id":"UKNjQ1AJG2g2"},"source":["# 2. Entrenamiento de un modelo"]},{"cell_type":"markdown","metadata":{"id":"nUq2yJzaAoNu"},"source":["## 2.1. Hiperparámetros para el entrenamiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkmY9d0il-O0"},"outputs":[],"source":["GAMMA              = 0.99\n","MEMORY_SIZE        = 1000000\n","LEARNING_RATE      = 0.001\n","BATCH_SIZE         = 64\n","EXPLORATION_MAX    = 1\n","EXPLORATION_MIN    = 0.01\n","EXPLORATION_DECAY  = 0.9999\n","NUMBER_OF_EPISODES = 1000\n","UPDATE_W           = 100\n","\n","#ARQUITECTURA\n","NEURONS_LAYER      = [128, 64, 64, 32]\n","ACT_OUT            = 'linear'\n","ACT_INTS           = 'relu'\n","\n","# Intervalo para grabar el vídeo generado por la ejecución\n","GRABAR_CADA_X = 20"]},{"cell_type":"markdown","metadata":{"id":"bR98eo8Hv2h0"},"source":["## 2.2. Creación del informe del modelo\n","En esta sección se crea un directorio en el que se guardará la configuración del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itzjrDgF6KnZ"},"outputs":[],"source":["res_path_vid = res_path + '/videos/'\n","if not os.path.isdir(res_path):\n","  os.mkdir(res_path)\n","  print(\"Created\")\n","else:\n","  print(\"Dir already exists\")\n","\n","if not os.path.isdir(res_path_vid):\n","  os.mkdir(res_path_vid)\n","  print(\"Created\")\n","else:\n","  print(\"Dir already exists\")\n","\n","data = {'Gamma': GAMMA, \n","        'Memory_size' : MEMORY_SIZE,\n","        'LR' : LEARNING_RATE,\n","        'Batch_size' : BATCH_SIZE,\n","        'Exploration_max' : EXPLORATION_MAX,\n","        'Exploration_min' : EXPLORATION_MIN,\n","        'Exploration_decay' : EXPLORATION_DECAY,\n","        'Episodes' :  NUMBER_OF_EPISODES,\n","        'Neuronas_por_capa' : '[' + \", \".join([str(i) for i in NEURONS_LAYER]) + ']',\n","        'Func_out' : ACT_OUT,\n","        'act_out': ACT_OUT,\n","        'act_int' : ACT_INTS,\n","        'TN_update_w': UPDATE_W,\n","        }\n","\n","df_model_conf = pd.DataFrame(data, index = [0])\n","df_model_conf.index = [str(dir)]\n","\n","\n","df_model_conf.to_excel(res_path + 'model_conf.xlsx', index=True)"]},{"cell_type":"markdown","metadata":{"id":"1FHL307DHACB"},"source":["## 2.3. Funciones *utils* para el entrenamiento del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Al2LqeZLcxWF"},"outputs":[],"source":["def guardar_resultados():\n","    #1-Guardamos el agente (instancia de la clase DQN)\n","    with open(res_path + 'agente', 'wb') as file:\n","      pickle.dump(agent, file)\n","\n","    #2-Guardamos el modelo directamente\n","    agent.model.save(res_path + 'model.h5')\n","\n","    #3-Guardamos los resultados del agente\n","    resultados = agent.get_resultados()\n","    episodios  = np.arange(len(resultados))\n","\n","    data = {'Episodio' : episodios,\n","            'Score' : resultados,\n","            'Exploration_rate' : exploration_rates,\n","            'Transitions' : transitions,\n","            'Goal_reach' : goal_episode\n","            }\n","\n","    df_resultados = pd.DataFrame(data)\n","    df_resultados.to_excel(res_path + 'resultados.xlsx', index=False)\n","    # df_resultados\n","\n","def guardar_resultados_goal_reached():\n","    #1-Creamos carpeta donde se guardan los resultados con el \n","    if not os.path.isdir(res_path_goal_reached):\n","      os.mkdir(res_path_goal_reached)\n","      print(\"Created\")\n","    else:\n","      print(\"Dir already exists\")\n","    \n","    #1-Guardamos el agente (instancia de la clase DQN)\n","    with open(res_path_goal_reached + 'agente', 'wb') as file:\n","      pickle.dump(agent, file)\n","\n","    #2-Guardamos el modelo directamente\n","    agent.model.save(res_path_goal_reached + 'model.h5')\n","\n","    #3-Guardamos los resultados del agente\n","    resultados = agent.get_resultados()\n","    episodios  = np.arange(len(resultados))\n","\n","    data = {'Episodio' : episodios,\n","            'Score' : resultados,\n","            'Exploration_rate' : exploration_rates,\n","            'Transitions' : transitions,\n","            'Goal_reach' : goal_episode\n","            }\n","\n","    df_resultados = pd.DataFrame(data)\n","    df_resultados.to_excel(res_path + 'resultados.xlsx', index=False)\n","    # df_resultados\n","\n","def create_environment():\n","    # Create simulated environment\n","    environment = wrap_env(gym.make(\"LunarLander-v2\"))\n","    number_of_observations = environment.observation_space.shape[0]\n","    number_of_actions = environment.action_space.n\n","    return environment, number_of_observations, number_of_actions\n","\n","def discretizar_estado(s):\n","    state = ( min(5, max(-5, np.round((s[0] / 0.1) * 2) / 2)), \\\n","              min(50, max(-10, np.round((s[1] / 0.1) * 2) / 2)), \\\n","              min(10, max(-10, np.round((s[2] / 0.1) * 2) / 2)), \\\n","              min(10, max(-10, np.round((s[3] / 0.1) * 2) / 2)), \\\n","              min(5, max(-5, np.round((s[4] / 0.1) * 2) / 2)), \\\n","              min(5, max(-5, np.round((s[5] / 0.1) * 2) / 2)), \\\n","              int(s[6]), \\\n","              int(s[7])\n","            )\n","\n","    return state\n","\n","def mostrar_resultados():\n","    resultados = agent.get_resultados()\n","    media_100 = []\n","    for episodio in range(len(resultados) + 1):\n","        datos = resultados[:episodio]\n","        media = np.round(np.mean(datos[-100:]), 2)\n","        media_100.append(media)\n","    plt.plot(resultados, 'c-')\n","    plt.plot(media_100, 'r-')\n","    plt.xlabel(\"Episode\")\n","    plt.ylabel(\"Score\")"]},{"cell_type":"markdown","metadata":{"id":"_G7_UU5vJzud"},"source":["## 2.4. Entrenamiento del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SufabET1mFXK"},"outputs":[],"source":["#Ruta para guardar los resultaddos en caso de que el modelo > 200 de avg en las ultimas 100\n","res_path_goal_reached = res_path + 'Goal_reached/'\n","# Creamos el entorno y el agente\n","environment, number_of_observations, number_of_actions = create_environment()\n","environment = gym.wrappers.Monitor(environment, res_path_vid, video_callable=lambda episode_id: episode_id % GRABAR_CADA_X == 0, force=True)\n","agent = DQN(number_of_observations, number_of_actions)\n","\n","episode = 0\n","updt_w = 0\n","exploration_rates = []\n","transitions = []\n","goal_episode = []\n","goal_reached = False\n","start_time = time.perf_counter()\n","avg_score_goal_reached = 0\n","\n","while (episode < NUMBER_OF_EPISODES) and not (goal_reached):\n","  if episode % 50 == 0:\n","    guardar_resultados()\n","\n","  episode += 1\n","  score = 0\n","  end_episode = False\n","  state = environment.reset()\n","  start = time.time()\n","  state = discretizar_estado(state)\n","\n","  while not(end_episode):\n","      # Seleccionamos una acción para el estado dado\n","      action = agent.select(state)\n","\n","      # Ejecutamos la acción en el entorno\n","      state_next, reward, terminal_state, info = environment.step(action)\n","\n","      # Discretizamos el estados\n","      state_next = discretizar_estado(state_next)\n","\n","      score += reward\n","\n","      # Guardamos en la memoria la transición (s,a,r,s') \n","      agent.remember(state, action, reward, state_next, terminal_state)\n","\n","      # Aprendizaje del modelo utilizando un batch de transiciones\n","      agent.learn()\n","      updt_w += 1\n","\n","      # Actualizamos los pesos de la target network si es necesario\n","      if updt_w >= UPDATE_W:\n","        agent.update_target_weights()\n","        updt_w = 0\n","\n","      # Detectamos el fin del episodio\n","      if terminal_state:\n","        agent.add_score(score)\n","        avg_score = np.mean(agent.get_resultados()[-100:])\n","\n","        end_episode = True \n","        exploration_rates.append(round(agent.exploration_rate, 2))\n","        transitions.append(str(agent.memory.current_size))\n","\n","        # Si la media de los últimos 100 episodios es 200 o más entonces el problema se considera resuelto\n","        if avg_score > avg_score_goal_reached:\n","            avg_score_goal_reached = avg_score\n","            #goal_reached = True\n","            goal_episode.append(1)\n","            guardar_resultados_goal_reached()\n","        else:\n","          goal_episode.append(0)\n","      \n","        print(\"Episode {0:>3}: \".format(episode), end = '')\n","        print(f\"last_reward: {f'{reward:+.2f}': >8} | \", end=\"\")\n","        print(f\"score: {f'{score:+.2f}': >8} | \", end=\"\")\n","        print(f\"avg score: {f'{avg_score:+.2f}': >8} | \", end=\"\")\n","        print(f\"time: {f'{ (time.time() - start)/60:+.2f}m': >8} | \", end=\"\")\n","        print(\"(exploration rate: %.2f, \" % agent.exploration_rate, end = '')\n","        print(\"transitions: \" + str(agent.memory.current_size) + \")\")\n","        \n","      else:\n","          state = state_next\n","\n","if goal_reached: print(\"Reached goal sucessfully.\")\n","else: print(\"Failure to reach the goal.\")\n","\n","print (\"Time:\", round((time.perf_counter() - start_time)/60), \"minutes\")\n","\n","mostrar_resultados()\n","environment.close()"]},{"cell_type":"markdown","metadata":{"id":"bOe_2DcwnlUZ"},"source":["# 3. Importación de un modelo entrenado"]},{"cell_type":"markdown","metadata":{"id":"otc6wu8CxnJq"},"source":["## 3.1. Función de carga del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQgm34A7xkOF"},"outputs":[],"source":["def cargar_modelo():\n","  with open(res_path_goal_reached + 'agente', 'rb') as file:\n","    agent = pickle.load(file)\n","  \n","  #Cargamos el modelo\n","  model = keras.models.load_model(res_path_goal_reached + 'model.h5')\n","  agent.model = model\n","\n","  return agent"]},{"cell_type":"markdown","metadata":{"id":"Dy4M9rH3un-D"},"source":["## 3.2. Funciones relativas al entorno y ejecución del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnbSpC7snz5j"},"outputs":[],"source":["def create_environment():\n","    # Create simulated environment\n","    environment = wrap_env(gym.make(\"LunarLander-v2\"))\n","    environment = gym.make(\"LunarLander-v2\")\n","    number_of_observations = environment.observation_space.shape[0]\n","    number_of_actions = environment.action_space.n\n","    return environment, number_of_observations, number_of_actions\n","\n","\n","def discretizar_estado(s):\n","    state = ( min(5, max(-5, np.round((s[0] / 0.1) * 2) / 2)), \\\n","              min(50, max(-10, np.round((s[1] / 0.1) * 2) / 2)), \\\n","              min(10, max(-10, np.round((s[2] / 0.1) * 2) / 2)), \\\n","              min(10, max(-10, np.round((s[3] / 0.1) * 2) / 2)), \\\n","              min(5, max(-5, np.round((s[4] / 0.1) * 2) / 2)), \\\n","              min(5, max(-5, np.round((s[5] / 0.1) * 2) / 2)), \\\n","              int(s[6]), \\\n","              int(s[7])\n","            )\n","\n","    return state\n","\n","def ejecutar_modelo(agent):\n","  environment, number_of_observations, number_of_actions = create_environment()\n","  environment = gym.wrappers.Monitor(environment, res_path_vid, video_callable=lambda episode_id: True, force=True)\n","  goal_reached = False\n","  end_episode = False\n","  score = 0\n","  state = environment.reset()\n","  state = discretizar_estado(state)\n","  start = time.time()\n","  start_time = time.perf_counter()\n","  while not(end_episode):\n","      # Select an action for the current state\n","      action = agent.select(state)\n","\n","      # Execute the action on the environment\n","      state_next, reward, terminal_state, info = environment.step(action)\n","\n","      # Discretizamos el estados\n","      state_next = discretizar_estado(state_next)\n","\n","      score += reward\n","    \n","      # Detect end of episode\n","      if terminal_state:\n","        if score >= 200:\n","            goal_reached = True\n","\n","        print(f\"last_reward: {f'{reward:+.2f}': >8} | \", end=\"\")\n","        print(f\"score: {f'{score:+.2f}': >8} | \", end=\"\")\n","        print(f\"time: {f'{ (time.time() - start)/60:+.2f}m': >8} | \", end=\"\")\n","        print(\"(transitions: \" + str(agent.memory.current_size) + \")\")\n","        end_episode = True \n","      else:\n","        state = state_next\n","\n","  if goal_reached: print(\"Reached goal sucessfully.\")\n","  else: print(\"Failure to reach the goal.\")\n","\n","  print (\"Time:\", round((time.perf_counter() - start_time)/60), \"minutes\")\n","\n","  environment.close()"]},{"cell_type":"markdown","metadata":{"id":"WvXOxWw3HeWN"},"source":["## 3.3. Función Main para importar, ejecutar el modelo y mostrar video de su funcionamiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjZx0w19ImZr"},"outputs":[],"source":["res_path_goal_reached = path + dir + \"/best_score\"\n","agente = cargar_modelo()\n","ejecutar_modelo(agente)\n","mostrar_videos()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["E96PZqWElzG-","bOe_2DcwnlUZ","Dy4M9rH3un-D"],"name":"DQN_LunarLander_Corregido.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}